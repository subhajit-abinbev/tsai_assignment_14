{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "torch.Size([4, 32, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Base Code\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# STOP\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[:1000]\n",
    "tokens = enc.encode(text)\n",
    "B, T = 4, 32\n",
    "buf = torch.tensor(tokens[:B*T + 1])\n",
    "buf = buf.to(device)\n",
    "x = buf[:-1].view(B, T)\n",
    "y = buf[1:].view(B, T)\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "logits = model(x)\n",
    "print(logits[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "tensor(10.9505, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Add loss\n",
    "# adding the batch loading part for training\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# STOP\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[:1000]\n",
    "tokens = enc.encode(text)\n",
    "B, T = 4, 32\n",
    "buf = torch.tensor(tokens[:B*T + 1])\n",
    "buf = buf.to(device)\n",
    "x = buf[:-1].view(B, T)\n",
    "y = buf[1:].view(B, T)\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "logits, loss = model(x, y)\n",
    "print(loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8249)\n"
     ]
    }
   ],
   "source": [
    "print(-torch.log(torch.tensor(1/50257)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step0, loss: 10.950464248657227\n",
      "step1, loss: 6.575095176696777\n",
      "step2, loss: 4.192558288574219\n",
      "step3, loss: 2.5733137130737305\n",
      "step4, loss: 1.4863554239273071\n",
      "step5, loss: 0.8406780362129211\n",
      "step6, loss: 0.46849799156188965\n",
      "step7, loss: 0.27460557222366333\n",
      "step8, loss: 0.17047762870788574\n",
      "step9, loss: 0.11028870940208435\n",
      "step10, loss: 0.07673893123865128\n",
      "step11, loss: 0.05782932788133621\n",
      "step12, loss: 0.04532918334007263\n",
      "step13, loss: 0.03701532632112503\n",
      "step14, loss: 0.03173821419477463\n",
      "step15, loss: 0.027637258172035217\n",
      "step16, loss: 0.023781869560480118\n",
      "step17, loss: 0.020386215299367905\n",
      "step18, loss: 0.017702696844935417\n",
      "step19, loss: 0.015623477287590504\n",
      "step20, loss: 0.013946694321930408\n",
      "step21, loss: 0.01253364235162735\n",
      "step22, loss: 0.011313318274915218\n",
      "step23, loss: 0.010254488326609135\n",
      "step24, loss: 0.009340804070234299\n",
      "step25, loss: 0.008557592518627644\n",
      "step26, loss: 0.007887445390224457\n",
      "step27, loss: 0.00731207150965929\n",
      "step28, loss: 0.006813973654061556\n",
      "step29, loss: 0.006377789191901684\n",
      "step30, loss: 0.005991082172840834\n",
      "step31, loss: 0.005644291639328003\n",
      "step32, loss: 0.005330744199454784\n",
      "step33, loss: 0.005045718047767878\n",
      "step34, loss: 0.004786138888448477\n",
      "step35, loss: 0.004549854900687933\n",
      "step36, loss: 0.0043351235799491405\n",
      "step37, loss: 0.004140518140047789\n",
      "step38, loss: 0.003964387811720371\n",
      "step39, loss: 0.003805305575951934\n",
      "step40, loss: 0.0036616805009543896\n",
      "step41, loss: 0.0035317738074809313\n",
      "step42, loss: 0.0034141656942665577\n",
      "step43, loss: 0.003307149512693286\n",
      "step44, loss: 0.0032093641348183155\n",
      "step45, loss: 0.003119415370747447\n",
      "step46, loss: 0.0030361919198185205\n",
      "step47, loss: 0.00295872800052166\n",
      "step48, loss: 0.002886295784264803\n",
      "step49, loss: 0.0028181325178593397\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "for i in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'step{i}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "loaded 338025 tokens\n",
      "1 epoch = 2640 batches\n",
      "step0, loss: 10.920181274414062\n",
      "step1, loss: 9.812644958496094\n",
      "step2, loss: 8.712605476379395\n",
      "step3, loss: 9.132890701293945\n",
      "step4, loss: 8.576388359069824\n",
      "step5, loss: 8.177474975585938\n",
      "step6, loss: 8.99504280090332\n",
      "step7, loss: 8.734390258789062\n",
      "step8, loss: 8.082488059997559\n",
      "step9, loss: 7.955374717712402\n",
      "step10, loss: 8.433320045471191\n",
      "step11, loss: 7.261044025421143\n",
      "step12, loss: 7.826310157775879\n",
      "step13, loss: 7.411554336547852\n",
      "step14, loss: 7.557544708251953\n",
      "step15, loss: 7.250461578369141\n",
      "step16, loss: 7.4031829833984375\n",
      "step17, loss: 8.335494995117188\n",
      "step18, loss: 7.099075794219971\n",
      "step19, loss: 7.762372016906738\n",
      "step20, loss: 7.483112335205078\n",
      "step21, loss: 7.6350812911987305\n",
      "step22, loss: 6.20756721496582\n",
      "step23, loss: 6.624934196472168\n",
      "step24, loss: 6.600099086761475\n",
      "step25, loss: 6.21558141708374\n",
      "step26, loss: 6.268570423126221\n",
      "step27, loss: 7.402144432067871\n",
      "step28, loss: 6.884303569793701\n",
      "step29, loss: 6.645229339599609\n",
      "step30, loss: 6.8692626953125\n",
      "step31, loss: 6.967392921447754\n",
      "step32, loss: 6.810275554656982\n",
      "step33, loss: 6.573600769042969\n",
      "step34, loss: 7.931281089782715\n",
      "step35, loss: 7.706600666046143\n",
      "step36, loss: 7.405459403991699\n",
      "step37, loss: 7.558559417724609\n",
      "step38, loss: 7.530642986297607\n",
      "step39, loss: 7.136451721191406\n",
      "step40, loss: 7.176370620727539\n",
      "step41, loss: 6.4121623039245605\n",
      "step42, loss: 6.538364410400391\n",
      "step43, loss: 6.688565731048584\n",
      "step44, loss: 6.657310962677002\n",
      "step45, loss: 6.642593860626221\n",
      "step46, loss: 5.681557655334473\n",
      "step47, loss: 5.988956451416016\n",
      "step48, loss: 6.86874532699585\n",
      "step49, loss: 6.56187629699707\n",
      "tensor(6.5619, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's make a full dataloadaer\n",
    "\n",
    "# DATALOADER\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# STOP\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2') \n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B*T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoaderLite(B = 4, T = 32)\n",
    "\n",
    "# NEW CODE\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "for i in range(50):\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'step{i}, loss: {loss.item()}')\n",
    "\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "loaded 338025 tokens\n",
      "1 epoch = 2640 batches\n",
      "step0, loss: 10.941183090209961\n",
      "step1, loss: 9.684093475341797\n",
      "step2, loss: 8.972075462341309\n",
      "step3, loss: 9.140128135681152\n",
      "step4, loss: 8.709209442138672\n",
      "step5, loss: 8.39765739440918\n",
      "step6, loss: 8.966639518737793\n",
      "step7, loss: 8.731046676635742\n",
      "step8, loss: 8.159562110900879\n",
      "step9, loss: 7.963054180145264\n",
      "step10, loss: 8.357766151428223\n",
      "step11, loss: 7.490817546844482\n",
      "step12, loss: 7.814469814300537\n",
      "step13, loss: 7.3949360847473145\n",
      "step14, loss: 7.5309062004089355\n",
      "step15, loss: 7.35502290725708\n",
      "step16, loss: 7.442122459411621\n",
      "step17, loss: 8.162090301513672\n",
      "step18, loss: 7.186564922332764\n",
      "step19, loss: 7.793060302734375\n",
      "step20, loss: 7.510186672210693\n",
      "step21, loss: 7.809110164642334\n",
      "step22, loss: 6.435649394989014\n",
      "step23, loss: 6.900519847869873\n",
      "step24, loss: 6.8828582763671875\n",
      "step25, loss: 6.716039657592773\n",
      "step26, loss: 6.772925853729248\n",
      "step27, loss: 7.5470991134643555\n",
      "step28, loss: 7.1985979080200195\n",
      "step29, loss: 6.984106540679932\n",
      "step30, loss: 6.989358901977539\n",
      "step31, loss: 7.272824764251709\n",
      "step32, loss: 7.173630714416504\n",
      "step33, loss: 6.975043296813965\n",
      "step34, loss: 7.928518772125244\n",
      "step35, loss: 7.814551830291748\n",
      "step36, loss: 7.689988613128662\n",
      "step37, loss: 7.679952144622803\n",
      "step38, loss: 7.953281879425049\n",
      "step39, loss: 7.438792705535889\n",
      "step40, loss: 7.39573335647583\n",
      "step41, loss: 6.936412811279297\n",
      "step42, loss: 7.070155143737793\n",
      "step43, loss: 7.0958356857299805\n",
      "step44, loss: 6.997889995574951\n",
      "step45, loss: 6.996677398681641\n",
      "step46, loss: 6.124850749969482\n",
      "step47, loss: 6.302637100219727\n",
      "step48, loss: 6.937215328216553\n",
      "step49, loss: 6.740163803100586\n",
      "tensor(6.7402, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Weight sharing\n",
    "# Weight Sharing\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# STOP\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2') \n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B*T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoaderLite(B = 4, T = 32)\n",
    "\n",
    "# NEW CODE\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "for i in range(50):\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'step{i}, loss: {loss.item()}')\n",
    "\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "loaded 338025 tokens\n",
      "1 epoch = 2640 batches\n",
      "step0, loss: 11.031367301940918\n",
      "step1, loss: 9.635494232177734\n",
      "step2, loss: 8.954357147216797\n",
      "step3, loss: 9.163911819458008\n",
      "step4, loss: 8.736703872680664\n",
      "step5, loss: 8.332362174987793\n",
      "step6, loss: 8.96406364440918\n",
      "step7, loss: 8.89919662475586\n",
      "step8, loss: 8.11715030670166\n",
      "step9, loss: 8.034951210021973\n",
      "step10, loss: 8.403307914733887\n",
      "step11, loss: 7.447668075561523\n",
      "step12, loss: 7.755180358886719\n",
      "step13, loss: 7.432129383087158\n",
      "step14, loss: 7.508581638336182\n",
      "step15, loss: 7.3706536293029785\n",
      "step16, loss: 7.445652008056641\n",
      "step17, loss: 8.286404609680176\n",
      "step18, loss: 7.155075550079346\n",
      "step19, loss: 7.8362226486206055\n",
      "step20, loss: 7.472085952758789\n",
      "step21, loss: 7.787292957305908\n",
      "step22, loss: 6.425104141235352\n",
      "step23, loss: 6.83209753036499\n",
      "step24, loss: 6.766149520874023\n",
      "step25, loss: 6.562461853027344\n",
      "step26, loss: 7.186689376831055\n",
      "step27, loss: 7.577374458312988\n",
      "step28, loss: 7.137980937957764\n",
      "step29, loss: 6.902638912200928\n",
      "step30, loss: 6.966521263122559\n",
      "step31, loss: 7.18404483795166\n",
      "step32, loss: 7.09447717666626\n",
      "step33, loss: 6.9601826667785645\n",
      "step34, loss: 7.892061710357666\n",
      "step35, loss: 7.736225128173828\n",
      "step36, loss: 7.610787391662598\n",
      "step37, loss: 7.592813968658447\n",
      "step38, loss: 7.806406497955322\n",
      "step39, loss: 10.698664665222168\n",
      "step40, loss: 7.427790641784668\n",
      "step41, loss: 6.9603776931762695\n",
      "step42, loss: 7.175405979156494\n",
      "step43, loss: 7.263674259185791\n",
      "step44, loss: 7.160301208496094\n",
      "step45, loss: 7.242332458496094\n",
      "step46, loss: 6.168982028961182\n",
      "step47, loss: 6.2597808837890625\n",
      "step48, loss: 6.9044294357299805\n",
      "step49, loss: 6.7513108253479\n",
      "tensor(6.7513, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Residual std scaling issue\n",
    "# Solving for residual std scaling issue\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# SEED\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# STOP\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2') \n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B*T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoaderLite(B = 4, T = 32)\n",
    "\n",
    "# NEW CODE\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "for i in range(50):\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'step{i}, loss: {loss.item()}')\n",
    "\n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed ups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "loaded 338025 tokens\n",
      "1 epoch = 41 batches\n",
      "step0 | loss: 10.974886894226074 | dt: 3860.74ms | tok/sec:  2121.87\n",
      "step1 | loss: 9.438068389892578 | dt: 2208.89ms | tok/sec:  3708.65\n",
      "step2 | loss: 9.426380157470703 | dt: 2256.86ms | tok/sec:  3629.82\n",
      "step3 | loss: 8.6160306930542 | dt: 2316.49ms | tok/sec:  3536.39\n",
      "step4 | loss: 8.432503700256348 | dt: 2347.10ms | tok/sec:  3490.27\n",
      "step5 | loss: 8.432765007019043 | dt: 2336.24ms | tok/sec:  3506.49\n",
      "step6 | loss: 8.32859992980957 | dt: 2329.66ms | tok/sec:  3516.39\n",
      "step7 | loss: 8.0079984664917 | dt: 2306.61ms | tok/sec:  3551.53\n",
      "step8 | loss: 7.736836910247803 | dt: 2328.86ms | tok/sec:  3517.60\n",
      "step9 | loss: 7.691933631896973 | dt: 2357.12ms | tok/sec:  3475.42\n",
      "step10 | loss: 7.659118175506592 | dt: 2346.67ms | tok/sec:  3490.90\n",
      "step11 | loss: 7.492031574249268 | dt: 2356.98ms | tok/sec:  3475.64\n",
      "step12 | loss: 7.4290595054626465 | dt: 2379.18ms | tok/sec:  3443.20\n",
      "step13 | loss: 7.214234352111816 | dt: 2418.43ms | tok/sec:  3387.32\n",
      "step14 | loss: 7.113617897033691 | dt: 2439.66ms | tok/sec:  3357.84\n",
      "step15 | loss: 6.933701515197754 | dt: 2447.53ms | tok/sec:  3347.04\n",
      "step16 | loss: 6.8333282470703125 | dt: 2459.06ms | tok/sec:  3331.36\n",
      "step17 | loss: 6.824620723724365 | dt: 2493.71ms | tok/sec:  3285.07\n",
      "step18 | loss: 6.695028781890869 | dt: 2485.01ms | tok/sec:  3296.57\n",
      "step19 | loss: 6.53237247467041 | dt: 2479.66ms | tok/sec:  3303.68\n",
      "step20 | loss: 6.570855140686035 | dt: 2528.30ms | tok/sec:  3240.12\n",
      "step21 | loss: 6.657503604888916 | dt: 2579.13ms | tok/sec:  3176.26\n",
      "step22 | loss: 6.550182819366455 | dt: 2584.31ms | tok/sec:  3169.90\n",
      "step23 | loss: 6.453341007232666 | dt: 2640.12ms | tok/sec:  3102.88\n",
      "step24 | loss: 6.436313152313232 | dt: 2643.21ms | tok/sec:  3099.27\n",
      "step25 | loss: 6.506349086761475 | dt: 2620.99ms | tok/sec:  3125.53\n",
      "step26 | loss: 6.599665641784668 | dt: 2585.60ms | tok/sec:  3168.32\n",
      "step27 | loss: 6.495684623718262 | dt: 2600.01ms | tok/sec:  3150.76\n",
      "step28 | loss: 6.7214884757995605 | dt: 2594.31ms | tok/sec:  3157.68\n",
      "step29 | loss: 6.525160312652588 | dt: 2598.08ms | tok/sec:  3153.10\n",
      "step30 | loss: 6.535007476806641 | dt: 2593.91ms | tok/sec:  3158.16\n",
      "step31 | loss: 6.529746055603027 | dt: 2647.17ms | tok/sec:  3094.63\n",
      "step32 | loss: 6.353812217712402 | dt: 2660.90ms | tok/sec:  3078.65\n",
      "step33 | loss: 6.7085957527160645 | dt: 2612.53ms | tok/sec:  3135.65\n",
      "step34 | loss: 6.56433629989624 | dt: 2547.47ms | tok/sec:  3215.73\n",
      "step35 | loss: 6.45352029800415 | dt: 2557.49ms | tok/sec:  3203.14\n",
      "step36 | loss: 6.5084428787231445 | dt: 2595.18ms | tok/sec:  3156.62\n",
      "step37 | loss: 6.511193752288818 | dt: 2538.61ms | tok/sec:  3226.96\n",
      "step38 | loss: 6.296462059020996 | dt: 2688.46ms | tok/sec:  3047.10\n",
      "step39 | loss: 6.338787078857422 | dt: 2592.89ms | tok/sec:  3159.41\n",
      "step40 | loss: 6.553049087524414 | dt: 2680.73ms | tok/sec:  3055.89\n",
      "step41 | loss: 6.309847354888916 | dt: 2616.89ms | tok/sec:  3130.43\n",
      "step42 | loss: 6.376102924346924 | dt: 2724.12ms | tok/sec:  3007.21\n",
      "step43 | loss: 6.1051435470581055 | dt: 2777.98ms | tok/sec:  2948.91\n",
      "step44 | loss: 6.0438008308410645 | dt: 2740.37ms | tok/sec:  2989.38\n",
      "step45 | loss: 6.116048812866211 | dt: 2715.47ms | tok/sec:  3016.78\n",
      "step46 | loss: 6.225061416625977 | dt: 2661.67ms | tok/sec:  3077.76\n",
      "step47 | loss: 6.189100742340088 | dt: 2664.57ms | tok/sec:  3074.42\n",
      "step48 | loss: 6.1041646003723145 | dt: 2638.02ms | tok/sec:  3105.36\n",
      "step49 | loss: 5.973804473876953 | dt: 2654.19ms | tok/sec:  3086.44\n",
      "tensor(5.9738, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# First Speed up.. just adding related calculation code\n",
    "# Solving for residual std scaling issue\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# SEED\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# STOP\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2') \n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B*T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoaderLite(B = 8, T = 1024)\n",
    "\n",
    "# NEW CODE\n",
    "import time\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "for i in range(50):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize() \n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "    print(f'step{i} | loss: {loss.item()} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec: .2f}')\n",
    "\n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "loaded 338025 tokens\n",
      "1 epoch = 41 batches\n",
      "step0 | loss: 10.9749174118042 | dt: 4586.91ms | tok/sec:  1785.95\n",
      "step1 | loss: 9.438058853149414 | dt: 1279.07ms | tok/sec:  6404.67\n",
      "step2 | loss: 9.425962448120117 | dt: 1325.97ms | tok/sec:  6178.13\n",
      "step3 | loss: 8.615828514099121 | dt: 1245.20ms | tok/sec:  6578.86\n",
      "step4 | loss: 8.43260383605957 | dt: 1301.69ms | tok/sec:  6293.34\n",
      "step5 | loss: 8.432779312133789 | dt: 1260.57ms | tok/sec:  6498.64\n",
      "step6 | loss: 8.328548431396484 | dt: 1274.78ms | tok/sec:  6426.19\n",
      "step7 | loss: 8.008028030395508 | dt: 1272.11ms | tok/sec:  6439.70\n",
      "step8 | loss: 7.736903190612793 | dt: 1267.27ms | tok/sec:  6464.28\n",
      "step9 | loss: 7.69196081161499 | dt: 1332.16ms | tok/sec:  6149.43\n",
      "step10 | loss: 7.659093379974365 | dt: 1338.21ms | tok/sec:  6121.63\n",
      "step11 | loss: 7.491988182067871 | dt: 1355.79ms | tok/sec:  6042.23\n",
      "step12 | loss: 7.429058074951172 | dt: 1304.50ms | tok/sec:  6279.80\n",
      "step13 | loss: 7.214259624481201 | dt: 1337.68ms | tok/sec:  6124.05\n",
      "step14 | loss: 7.113620758056641 | dt: 1312.46ms | tok/sec:  6241.72\n",
      "step15 | loss: 6.933676242828369 | dt: 1308.87ms | tok/sec:  6258.85\n",
      "step16 | loss: 6.833273887634277 | dt: 1335.27ms | tok/sec:  6135.11\n",
      "step17 | loss: 6.824584484100342 | dt: 1345.15ms | tok/sec:  6090.01\n",
      "step18 | loss: 6.694973945617676 | dt: 1359.73ms | tok/sec:  6024.73\n",
      "step19 | loss: 6.532279014587402 | dt: 1321.93ms | tok/sec:  6197.00\n",
      "step20 | loss: 6.570504665374756 | dt: 1388.17ms | tok/sec:  5901.29\n",
      "step21 | loss: 6.713894844055176 | dt: 1409.12ms | tok/sec:  5813.55\n",
      "step22 | loss: 6.550199031829834 | dt: 1406.64ms | tok/sec:  5823.83\n",
      "step23 | loss: 6.453551769256592 | dt: 1408.82ms | tok/sec:  5814.81\n",
      "step24 | loss: 6.436790943145752 | dt: 1417.59ms | tok/sec:  5778.81\n",
      "step25 | loss: 6.5069122314453125 | dt: 1425.46ms | tok/sec:  5746.91\n",
      "step26 | loss: 6.59920072555542 | dt: 1400.08ms | tok/sec:  5851.10\n",
      "step27 | loss: 6.49519681930542 | dt: 1375.44ms | tok/sec:  5955.90\n",
      "step28 | loss: 6.719662666320801 | dt: 1360.79ms | tok/sec:  6020.03\n",
      "step29 | loss: 6.524350643157959 | dt: 1331.21ms | tok/sec:  6153.80\n",
      "step30 | loss: 6.535549640655518 | dt: 1343.75ms | tok/sec:  6096.35\n",
      "step31 | loss: 6.5321946144104 | dt: 1347.10ms | tok/sec:  6081.21\n",
      "step32 | loss: 6.358680725097656 | dt: 1358.33ms | tok/sec:  6030.93\n",
      "step33 | loss: 6.70836877822876 | dt: 1367.57ms | tok/sec:  5990.19\n",
      "step34 | loss: 6.564638137817383 | dt: 1437.94ms | tok/sec:  5697.03\n",
      "step35 | loss: 6.4550981521606445 | dt: 1438.25ms | tok/sec:  5695.82\n",
      "step36 | loss: 6.511226654052734 | dt: 1410.50ms | tok/sec:  5807.87\n",
      "step37 | loss: 6.514230728149414 | dt: 1398.60ms | tok/sec:  5857.28\n",
      "step38 | loss: 6.300897121429443 | dt: 1428.32ms | tok/sec:  5735.39\n",
      "step39 | loss: 6.344309329986572 | dt: 1415.27ms | tok/sec:  5788.31\n",
      "step40 | loss: 6.5587921142578125 | dt: 1460.96ms | tok/sec:  5607.27\n",
      "step41 | loss: 6.308258533477783 | dt: 1487.89ms | tok/sec:  5505.79\n",
      "step42 | loss: 6.373755931854248 | dt: 1433.38ms | tok/sec:  5715.18\n",
      "step43 | loss: 6.105081081390381 | dt: 1454.84ms | tok/sec:  5630.87\n",
      "step44 | loss: 6.0435872077941895 | dt: 1482.83ms | tok/sec:  5524.57\n",
      "step45 | loss: 6.117457389831543 | dt: 1449.73ms | tok/sec:  5650.69\n",
      "step46 | loss: 6.226179122924805 | dt: 1494.97ms | tok/sec:  5479.72\n",
      "step47 | loss: 6.1879448890686035 | dt: 1482.71ms | tok/sec:  5525.00\n",
      "step48 | loss: 6.105529308319092 | dt: 1505.53ms | tok/sec:  5441.27\n",
      "step49 | loss: 5.977785587310791 | dt: 1556.06ms | tok/sec:  5264.59\n",
      "tensor(5.9778, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# we are only going to add 'high'\n",
    "# torch.set_float32_matmul_precision('high')\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# SEED\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# STOP\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2') \n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B*T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoaderLite(B = 8, T = 1024)\n",
    "\n",
    "# NEW CODE\n",
    "import time\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "for i in range(50):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # torch.cuda.synchronize() \n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "    print(f'step{i} | loss: {loss.item()} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec: .2f}')\n",
    "\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# step13 | loss: 7.214234352111816 | dt: 2418.43ms | tok/sec:  3387.32\n",
    "# step13 | loss: 7.214259624481201 | dt: 1337.68ms | tok/sec:  6124.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "loaded 338025 tokens\n",
      "1 epoch = 41 batches\n",
      "step0 | loss: 10.946540832519531 | dt: 3838.33ms | tok/sec:  2134.26\n",
      "step1 | loss: 9.497331619262695 | dt: 985.99ms | tok/sec:  8308.39\n",
      "step2 | loss: 10.459564208984375 | dt: 967.06ms | tok/sec:  8471.06\n",
      "step3 | loss: 9.196014404296875 | dt: 956.78ms | tok/sec:  8562.05\n",
      "step4 | loss: 8.783353805541992 | dt: 967.49ms | tok/sec:  8467.29\n",
      "step5 | loss: 8.550586700439453 | dt: 961.14ms | tok/sec:  8523.24\n",
      "step6 | loss: 8.453838348388672 | dt: 946.04ms | tok/sec:  8659.27\n",
      "step7 | loss: 8.166803359985352 | dt: 973.36ms | tok/sec:  8416.21\n",
      "step8 | loss: 7.900913238525391 | dt: 966.09ms | tok/sec:  8479.55\n",
      "step9 | loss: 7.84703254699707 | dt: 957.49ms | tok/sec:  8555.67\n",
      "step10 | loss: 7.785855293273926 | dt: 978.80ms | tok/sec:  8369.42\n",
      "step11 | loss: 7.580438613891602 | dt: 981.27ms | tok/sec:  8348.39\n",
      "step12 | loss: 7.539167404174805 | dt: 995.78ms | tok/sec:  8226.73\n",
      "step13 | loss: 7.330005645751953 | dt: 978.19ms | tok/sec:  8374.65\n",
      "step14 | loss: 7.246393203735352 | dt: 980.05ms | tok/sec:  8358.77\n",
      "step15 | loss: 7.049274444580078 | dt: 1006.64ms | tok/sec:  8137.93\n",
      "step16 | loss: 6.943470001220703 | dt: 983.38ms | tok/sec:  8330.47\n",
      "step17 | loss: 6.921173095703125 | dt: 989.64ms | tok/sec:  8277.72\n",
      "step18 | loss: 6.785503387451172 | dt: 1000.51ms | tok/sec:  8187.80\n",
      "step19 | loss: 6.645259857177734 | dt: 996.81ms | tok/sec:  8218.25\n",
      "step20 | loss: 6.677854537963867 | dt: 998.26ms | tok/sec:  8206.29\n",
      "step21 | loss: 6.559115409851074 | dt: 1001.23ms | tok/sec:  8181.92\n",
      "step22 | loss: 6.650574684143066 | dt: 993.54ms | tok/sec:  8245.23\n",
      "step23 | loss: 6.5437726974487305 | dt: 991.71ms | tok/sec:  8260.46\n",
      "step24 | loss: 6.519322395324707 | dt: 1020.09ms | tok/sec:  8030.66\n",
      "step25 | loss: 6.5783843994140625 | dt: 1015.88ms | tok/sec:  8063.96\n",
      "step26 | loss: 6.664438247680664 | dt: 1020.74ms | tok/sec:  8025.58\n",
      "step27 | loss: 6.562126159667969 | dt: 990.91ms | tok/sec:  8267.15\n",
      "step28 | loss: 6.769287109375 | dt: 1011.42ms | tok/sec:  8099.53\n",
      "step29 | loss: 6.590570449829102 | dt: 1023.58ms | tok/sec:  8003.27\n",
      "step30 | loss: 6.5765790939331055 | dt: 1035.58ms | tok/sec:  7910.56\n",
      "step31 | loss: 6.593608856201172 | dt: 1038.60ms | tok/sec:  7887.58\n",
      "step32 | loss: 6.47141170501709 | dt: 1055.34ms | tok/sec:  7762.44\n",
      "step33 | loss: 6.750811576843262 | dt: 1052.39ms | tok/sec:  7784.15\n",
      "step34 | loss: 6.625387191772461 | dt: 1030.54ms | tok/sec:  7949.25\n",
      "step35 | loss: 6.51841926574707 | dt: 1049.47ms | tok/sec:  7805.85\n",
      "step36 | loss: 6.579293727874756 | dt: 1046.57ms | tok/sec:  7827.46\n",
      "step37 | loss: 6.5731401443481445 | dt: 1017.80ms | tok/sec:  8048.77\n",
      "step38 | loss: 6.324164390563965 | dt: 1031.65ms | tok/sec:  7940.70\n",
      "step39 | loss: 6.409440517425537 | dt: 999.10ms | tok/sec:  8199.40\n",
      "step40 | loss: 6.608316421508789 | dt: 1009.21ms | tok/sec:  8117.20\n",
      "step41 | loss: 6.347144603729248 | dt: 1025.63ms | tok/sec:  7987.32\n",
      "step42 | loss: 6.412768840789795 | dt: 1041.75ms | tok/sec:  7863.73\n",
      "step43 | loss: 6.1114115715026855 | dt: 1046.72ms | tok/sec:  7826.34\n",
      "step44 | loss: 6.053976058959961 | dt: 1049.52ms | tok/sec:  7805.45\n",
      "step45 | loss: 6.128930568695068 | dt: 1085.39ms | tok/sec:  7547.52\n",
      "step46 | loss: 6.228908061981201 | dt: 1084.25ms | tok/sec:  7555.47\n",
      "step47 | loss: 6.180782318115234 | dt: 1081.49ms | tok/sec:  7574.72\n",
      "step48 | loss: 6.071769714355469 | dt: 1104.45ms | tok/sec:  7417.30\n",
      "step49 | loss: 5.936774253845215 | dt: 1105.64ms | tok/sec:  7409.29\n",
      "tensor(5.9368, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# now we'll add autocast to the forward pass\n",
    "# Logits and Loss\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# SEED\n",
    "# torch.manual_seed(1337)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(1337)\n",
    "\n",
    "# STOP\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2') \n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B*T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoaderLite(B = 8, T = 1024)\n",
    "\n",
    "# NEW CODE\n",
    "import time\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "for i in range(50):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # torch.cuda.synchronize() \n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "    print(f'step{i} | loss: {loss.item()} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec: .2f}')\n",
    "\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# step13 | loss: 7.214234352111816 | dt: 2418.43ms | tok/sec:  3387.32\n",
    "# step13 | loss: 7.214259624481201 | dt: 1337.68ms | tok/sec:  6124.05\n",
    "# step13 | loss: 7.330005645751953 | dt: 978.19ms | tok/sec:  8374.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Windows not yet supported for torch.compile",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36176\\2533983332.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGPT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGPTConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoaderLite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Installs\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"inductor\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_TorchCompileInductorWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnopython\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfullgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Installs\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(backend, nopython, guard_export_fn, guard_fail_fn, disable, dynamic)\u001b[0m\n\u001b[0;32m    411\u001b[0m             \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m     \"\"\"\n\u001b[1;32m--> 413\u001b[1;33m     \u001b[0mcheck_if_dynamo_supported\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m     \u001b[1;31m# Note: The hooks object could be global instead of passed around, *however* that would make\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;31m# for a confusing API usage and plumbing story wherein we nest multiple .optimize calls.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Installs\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py\u001b[0m in \u001b[0;36mcheck_if_dynamo_supported\u001b[1;34m()\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcheck_if_dynamo_supported\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"win32\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Windows not yet supported for torch.compile\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Python 3.11+ not yet supported for torch.compile\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Windows not yet supported for torch.compile"
     ]
    }
   ],
   "source": [
    "# Next is torch.compile that doesn't work on mac or windows yet\n",
    "# torch.compile\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal = True) \n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# SEED\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# STOP\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2') \n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B*T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "train_loader = DataLoaderLite(B = 8, T = 1024)\n",
    "\n",
    "# NEW CODE\n",
    "import time\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "for i in range(50):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # torch.cuda.synchronize() \n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "    print(f'step{i} | loss: {loss.item()} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec: .2f}')\n",
    "\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# step13 | loss: 7.214234352111816 | dt: 2418.43ms | tok/sec:  3387.32\n",
    "# step13 | loss: 7.214259624481201 | dt: 1337.68ms | tok/sec:  6124.05\n",
    "# step13 | loss: 7.330005645751953 | dt: 978.19ms | tok/sec:  8374.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "loaded 338025 tokens\n",
      "1 epoch = 20 batches\n",
      "step0 | loss: 10.968460083007812 | dt: 2102.47ms | tok/sec:  7792.75\n",
      "step1 | loss: 9.277786254882812 | dt: 984.27ms | tok/sec:  16645.76\n",
      "step2 | loss: 9.362764358520508 | dt: 977.83ms | tok/sec:  16755.52\n",
      "step3 | loss: 8.810545921325684 | dt: 1007.94ms | tok/sec:  16254.97\n",
      "step4 | loss: 8.556181907653809 | dt: 1006.36ms | tok/sec:  16280.48\n",
      "step5 | loss: 8.501791954040527 | dt: 1004.24ms | tok/sec:  16314.90\n",
      "step6 | loss: 8.319710731506348 | dt: 997.48ms | tok/sec:  16425.39\n",
      "step7 | loss: 8.10116958618164 | dt: 1020.09ms | tok/sec:  16061.35\n",
      "step8 | loss: 7.837139129638672 | dt: 1014.12ms | tok/sec:  16155.81\n",
      "step9 | loss: 7.592104911804199 | dt: 1023.97ms | tok/sec:  16000.48\n",
      "step10 | loss: 7.407554626464844 | dt: 1026.98ms | tok/sec:  15953.58\n",
      "step11 | loss: 7.267879486083984 | dt: 1026.57ms | tok/sec:  15959.92\n",
      "step12 | loss: 7.089778900146484 | dt: 1027.40ms | tok/sec:  15947.03\n",
      "step13 | loss: 7.012979507446289 | dt: 1032.97ms | tok/sec:  15861.12\n",
      "step14 | loss: 6.943789005279541 | dt: 1021.67ms | tok/sec:  16036.48\n",
      "step15 | loss: 6.776766300201416 | dt: 1030.95ms | tok/sec:  15892.19\n",
      "step16 | loss: 6.745838165283203 | dt: 1038.94ms | tok/sec:  15769.88\n",
      "step17 | loss: 6.702724456787109 | dt: 1018.05ms | tok/sec:  16093.47\n",
      "step18 | loss: 6.646297454833984 | dt: 1030.54ms | tok/sec:  15898.42\n",
      "step19 | loss: 6.46529483795166 | dt: 1027.34ms | tok/sec:  15948.00\n",
      "step20 | loss: 6.4143877029418945 | dt: 1036.32ms | tok/sec:  15809.79\n",
      "step21 | loss: 6.1758503913879395 | dt: 1064.00ms | tok/sec:  15398.48\n",
      "step22 | loss: 6.264840126037598 | dt: 1055.32ms | tok/sec:  15525.09\n",
      "step23 | loss: 6.181435585021973 | dt: 1061.12ms | tok/sec:  15440.36\n",
      "step24 | loss: 6.1195068359375 | dt: 1048.66ms | tok/sec:  15623.73\n",
      "step25 | loss: 6.3361663818359375 | dt: 1064.71ms | tok/sec:  15388.28\n",
      "step26 | loss: 6.437063694000244 | dt: 1055.26ms | tok/sec:  15526.04\n",
      "step27 | loss: 6.320858001708984 | dt: 1066.23ms | tok/sec:  15366.29\n",
      "step28 | loss: 6.248010158538818 | dt: 1076.07ms | tok/sec:  15225.75\n",
      "step29 | loss: 6.130649566650391 | dt: 1076.81ms | tok/sec:  15215.28\n",
      "step30 | loss: 6.1267991065979 | dt: 1086.26ms | tok/sec:  15082.89\n",
      "step31 | loss: 6.152615547180176 | dt: 1100.18ms | tok/sec:  14892.06\n",
      "step32 | loss: 6.078314304351807 | dt: 1085.94ms | tok/sec:  15087.37\n",
      "step33 | loss: 6.204565525054932 | dt: 1116.10ms | tok/sec:  14679.63\n",
      "step34 | loss: 6.311124801635742 | dt: 1089.85ms | tok/sec:  15033.29\n",
      "step35 | loss: 6.143596649169922 | dt: 1074.69ms | tok/sec:  15245.31\n",
      "step36 | loss: 6.125319480895996 | dt: 1074.17ms | tok/sec:  15252.71\n",
      "step37 | loss: 6.113572120666504 | dt: 1061.42ms | tok/sec:  15435.93\n",
      "step38 | loss: 6.0996479988098145 | dt: 1077.62ms | tok/sec:  15203.95\n",
      "step39 | loss: 5.946081161499023 | dt: 1073.26ms | tok/sec:  15265.68\n",
      "step40 | loss: 6.125596046447754 | dt: 1064.67ms | tok/sec:  15388.82\n",
      "step41 | loss: 5.846945285797119 | dt: 1103.51ms | tok/sec:  14847.14\n",
      "step42 | loss: 6.1102776527404785 | dt: 1072.32ms | tok/sec:  15279.08\n",
      "step43 | loss: 5.916009902954102 | dt: 1076.30ms | tok/sec:  15222.47\n",
      "step44 | loss: 5.870513439178467 | dt: 1093.51ms | tok/sec:  14982.89\n",
      "step45 | loss: 6.1130900382995605 | dt: 1078.92ms | tok/sec:  15185.49\n",
      "step46 | loss: 6.216333389282227 | dt: 1078.63ms | tok/sec:  15189.58\n",
      "step47 | loss: 6.078805923461914 | dt: 1121.00ms | tok/sec:  14615.46\n",
      "step48 | loss: 6.0154619216918945 | dt: 1078.17ms | tok/sec:  15196.07\n",
      "step49 | loss: 5.952970504760742 | dt: 1093.62ms | tok/sec:  14981.50\n",
      "tensor(5.9530, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Flash Attention\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal = True) # Flash attention\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# SEED\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# STOP\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2') \n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B*T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "# model = torch.compile(model)\n",
    "\n",
    "train_loader = DataLoaderLite(B = 16, T = 1024)\n",
    "\n",
    "# NEW CODE\n",
    "import time\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "for i in range(50):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # torch.cuda.synchronize() \n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "    print(f'step{i} | loss: {loss.item()} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec: .2f}')\n",
    "\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# step13 | loss: 7.214234352111816 | dt: 2418.43ms | tok/sec:  3387.32\n",
    "# step13 | loss: 7.214259624481201 | dt: 1337.68ms | tok/sec:  6124.05\n",
    "# step13 | loss: 7.330005645751953 | dt: 978.19ms | tok/sec:  8374.65\n",
    "# step13 | loss: 7.012979507446289 | dt: 1032.97ms | tok/sec:  15861.12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "loaded 338025 tokens\n",
      "1 epoch = 20 batches\n",
      "step0 | loss: 10.975273132324219 | dt: 2864.13ms | tok/sec:  5720.41\n",
      "step1 | loss: 9.234769821166992 | dt: 970.75ms | tok/sec:  16877.62\n",
      "step2 | loss: 9.308356285095215 | dt: 969.80ms | tok/sec:  16894.27\n",
      "step3 | loss: 8.818010330200195 | dt: 975.39ms | tok/sec:  16797.31\n",
      "step4 | loss: 8.566673278808594 | dt: 986.90ms | tok/sec:  16601.50\n",
      "step5 | loss: 8.494190216064453 | dt: 984.98ms | tok/sec:  16633.76\n",
      "step6 | loss: 8.315021514892578 | dt: 988.41ms | tok/sec:  16576.05\n",
      "step7 | loss: 8.09047794342041 | dt: 993.04ms | tok/sec:  16498.75\n",
      "step8 | loss: 7.825098037719727 | dt: 998.08ms | tok/sec:  16415.45\n",
      "step9 | loss: 7.591974258422852 | dt: 987.66ms | tok/sec:  16588.64\n",
      "step10 | loss: 7.371957302093506 | dt: 1018.85ms | tok/sec:  16080.90\n",
      "step11 | loss: 7.238385200500488 | dt: 1013.84ms | tok/sec:  16160.36\n",
      "step12 | loss: 7.02282190322876 | dt: 1041.97ms | tok/sec:  15724.14\n",
      "step13 | loss: 6.9644927978515625 | dt: 1004.06ms | tok/sec:  16317.75\n",
      "step14 | loss: 6.9268951416015625 | dt: 1035.19ms | tok/sec:  15827.06\n",
      "step15 | loss: 6.735766410827637 | dt: 1024.06ms | tok/sec:  15999.00\n",
      "step16 | loss: 6.683448791503906 | dt: 1025.18ms | tok/sec:  15981.59\n",
      "step17 | loss: 6.608946800231934 | dt: 1031.83ms | tok/sec:  15878.57\n",
      "step18 | loss: 6.573506832122803 | dt: 1049.93ms | tok/sec:  15604.81\n",
      "step19 | loss: 6.396153450012207 | dt: 1033.15ms | tok/sec:  15858.36\n",
      "step20 | loss: 6.384952545166016 | dt: 1028.36ms | tok/sec:  15932.10\n",
      "step21 | loss: 6.144393444061279 | dt: 1023.87ms | tok/sec:  16001.98\n",
      "step22 | loss: 6.258566856384277 | dt: 1037.64ms | tok/sec:  15789.64\n",
      "step23 | loss: 6.1380391120910645 | dt: 1043.67ms | tok/sec:  15698.40\n",
      "step24 | loss: 6.093527793884277 | dt: 1032.21ms | tok/sec:  15872.75\n",
      "step25 | loss: 6.32279109954834 | dt: 1032.52ms | tok/sec:  15867.91\n",
      "step26 | loss: 6.399025917053223 | dt: 1042.45ms | tok/sec:  15716.87\n",
      "step27 | loss: 6.292740345001221 | dt: 1051.07ms | tok/sec:  15587.95\n",
      "step28 | loss: 6.213570594787598 | dt: 1065.28ms | tok/sec:  15379.93\n",
      "step29 | loss: 6.112643718719482 | dt: 1063.98ms | tok/sec:  15398.82\n",
      "step30 | loss: 6.084699630737305 | dt: 1068.93ms | tok/sec:  15327.55\n",
      "step31 | loss: 6.1366496086120605 | dt: 1072.00ms | tok/sec:  15283.54\n",
      "step32 | loss: 6.069981098175049 | dt: 1074.38ms | tok/sec:  15249.67\n",
      "step33 | loss: 6.182361602783203 | dt: 1096.13ms | tok/sec:  14947.13\n",
      "step34 | loss: 6.298973083496094 | dt: 1081.58ms | tok/sec:  15148.19\n",
      "step35 | loss: 6.129631996154785 | dt: 1081.32ms | tok/sec:  15151.89\n",
      "step36 | loss: 6.102990627288818 | dt: 1100.68ms | tok/sec:  14885.38\n",
      "step37 | loss: 6.088913917541504 | dt: 1107.82ms | tok/sec:  14789.40\n",
      "step38 | loss: 6.075800895690918 | dt: 1102.93ms | tok/sec:  14854.97\n",
      "step39 | loss: 5.914186477661133 | dt: 1096.10ms | tok/sec:  14947.55\n",
      "step40 | loss: 6.108285427093506 | dt: 1132.54ms | tok/sec:  14466.61\n",
      "step41 | loss: 5.870655536651611 | dt: 1130.99ms | tok/sec:  14486.38\n",
      "step42 | loss: 5.992389678955078 | dt: 1146.24ms | tok/sec:  14293.72\n",
      "step43 | loss: 5.916553020477295 | dt: 1133.09ms | tok/sec:  14459.56\n",
      "step44 | loss: 5.8555707931518555 | dt: 1155.05ms | tok/sec:  14184.61\n",
      "step45 | loss: 6.068148612976074 | dt: 1154.51ms | tok/sec:  14191.33\n",
      "step46 | loss: 6.192309379577637 | dt: 1136.36ms | tok/sec:  14417.99\n",
      "step47 | loss: 6.065110206604004 | dt: 1135.67ms | tok/sec:  14426.72\n",
      "step48 | loss: 5.989673614501953 | dt: 1154.58ms | tok/sec:  14190.41\n",
      "step49 | loss: 5.907505035400391 | dt: 1161.04ms | tok/sec:  14111.45\n",
      "tensor(5.9075, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# POwer of 2\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal = True) # Flash attention\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50304 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# SEED\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# STOP\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2') \n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B*T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "# model = torch.compile(model)\n",
    "\n",
    "train_loader = DataLoaderLite(B = 16, T = 1024)\n",
    "\n",
    "# NEW CODE\n",
    "import time\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "for i in range(50):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # torch.cuda.synchronize() \n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "    print(f'step{i} | loss: {loss.item()} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec: .2f}')\n",
    "\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# step13 | loss: 7.214234352111816 | dt: 2418.43ms | tok/sec:  3387.32\n",
    "# step13 | loss: 7.214259624481201 | dt: 1337.68ms | tok/sec:  6124.05\n",
    "# step13 | loss: 7.330005645751953 | dt: 978.19ms | tok/sec:  8374.65\n",
    "# step13 | loss: 7.012979507446289 | dt: 1032.97ms | tok/sec:  15861.12\n",
    "# step13 | loss: 6.9644927978515625 | dt: 1004.06ms | tok/sec:  16317.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "didn't crash yet!\n",
      "tensor(10.9279, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Profiling\n",
    "# Add loss\n",
    "# adding the batch loading part for training\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "print(\"didn't crash yet!\")\n",
    "# STOP\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[:1000]\n",
    "tokens = enc.encode(text)\n",
    "B, T = 4, 32\n",
    "buf = torch.tensor(tokens[:B*T + 1])\n",
    "buf = buf.to(device)\n",
    "x = buf[:-1].view(B, T)\n",
    "y = buf[1:].view(B, T)\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "logits, loss = model(x, y)\n",
    "print(loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "               aten::arange         2.33%       1.381ms         4.75%       2.814ms       1.407ms           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                aten::empty        10.50%       6.218ms        10.50%       6.218ms      45.058us           0 b           0 b      29.65 Mb      29.65 Mb           138  \n",
      "              aten::resize_         0.75%     446.000us         0.75%     446.000us     148.667us           0 b           0 b     480.50 Kb     480.50 Kb             3  \n",
      "            aten::embedding         1.55%     915.000us         2.88%       1.704ms     852.000us           0 b           0 b     480.00 Kb           0 b             2  \n",
      "         aten::index_select         0.45%     269.000us         0.56%     330.000us     165.000us           0 b           0 b     480.00 Kb           0 b             2  \n",
      "              aten::reshape         1.80%       1.064ms        15.86%       9.393ms     187.860us           0 b           0 b      13.50 Mb           0 b            50  \n",
      "       aten::_reshape_alias         0.26%     156.000us         0.26%     156.000us      11.143us           0 b           0 b           0 b           0 b            14  \n",
      "                 aten::view         2.86%       1.695ms         2.86%       1.695ms       8.692us           0 b           0 b           0 b           0 b           195  \n",
      "                  aten::add         7.44%       4.404ms         7.44%       4.404ms     176.160us           0 b           0 b       9.38 Mb       9.38 Mb            25  \n",
      "           aten::layer_norm         1.25%     740.000us         7.08%       4.192ms     167.680us           0 b           0 b       9.40 Mb           0 b            25  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 59.221ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = model\n",
    "inputs = x\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU],\n",
    "        profile_memory=True, record_shapes=True) as prof:\n",
    "    model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n",
    "prof.export_chrome_trace(\"trace.json\")\n",
    "# chrome://tracing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total KFLOPs  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                aten::addmm        12.08%      10.050ms        14.28%      11.881ms     247.521us      16.915ms        14.22%      22.312ms     464.833us            48  21743271.936  \n",
      "                   aten::mm         0.16%     136.000us         0.16%     136.000us     136.000us      19.705ms        16.57%      19.705ms      19.705ms             1   9880928.256  \n",
      "                  aten::bmm         4.63%       3.849ms         4.63%       3.849ms     160.375us       3.370ms         2.83%       3.370ms     140.417us            24    150994.944  \n",
      "                  aten::add         1.55%       1.287ms         1.55%       1.287ms      51.480us     845.000us         0.71%     845.000us      33.800us            25      2457.600  \n",
      "                  aten::mul         0.58%     484.000us         0.58%     484.000us      40.333us     277.000us         0.23%     277.000us      23.083us            12       589.824  \n",
      "               aten::arange         0.20%     167.000us         0.32%     270.000us     135.000us      43.000us         0.04%      85.000us      42.500us             2            --  \n",
      "                aten::empty         1.19%     991.000us         1.19%     991.000us       7.181us       1.997ms         1.68%       1.997ms      14.471us           138            --  \n",
      "              aten::resize_         0.03%      26.000us         0.03%      26.000us       8.667us      16.000us         0.01%      16.000us       5.333us             3            --  \n",
      "            aten::embedding         0.13%     109.000us         0.32%     267.000us     133.500us      41.000us         0.03%     384.000us     192.000us             2            --  \n",
      "         aten::index_select         0.13%     111.000us         0.16%     129.000us      64.500us     218.000us         0.18%     306.000us     153.000us             2            --  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 83.213ms\n",
      "Self CUDA time total: 118.925ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    with_flops=True\n",
    ") as prof:\n",
    "    model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"flops\", row_limit=10))\n",
    "prof.export_chrome_trace(\"trace.json\")\n",
    "# chrome://tracing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Memory and Compute Requirement notebook\n",
    "\n",
    "This notebook is a simple tool to estimate the memory and compute requirements for the solution of the problem. The notebook is divided into two sections: the first one is dedicated to the memory requirements, while the second one is dedicated to the compute requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embeddings               38,597,376      31.24%\n",
      "Attention                      28,311,552      22.91%\n",
      "MLP                            56,623,104      45.83%\n",
      "Norm                               19,200       0.02%\n",
      "Output                                  0       0.00%\n",
      "Total parameters: 123,551,232\n"
     ]
    }
   ],
   "source": [
    "# Some working statistics\n",
    "\n",
    "# Model\n",
    "vocabulary_size = 50257\n",
    "embedding_dimensions = 768\n",
    "num_attention_heads = 12\n",
    "num_hidden_layers = 12\n",
    "feed_forward_ratio = 4\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "parameter_counts = {\n",
    "    \"Token Embeddings\": vocabulary_size * embedding_dimensions,\n",
    "    \"Attention\": (\n",
    "        embedding_dimensions * 3 * embedding_dimensions + embedding_dimensions**2\n",
    "    ) * num_hidden_layers,\n",
    "    \"MLP\": (\n",
    "        embedding_dimensions * feed_forward_ratio * embedding_dimensions * 2\n",
    "    ) * num_hidden_layers,\n",
    "    \"Norm\": embedding_dimensions * 2 * num_hidden_layers + embedding_dimensions,\n",
    "    \"Output\": 0, # We share the embedding weights\n",
    "}\n",
    "\n",
    "plt.bar(parameter_counts.keys(), parameter_counts.values())\n",
    "\n",
    "plt.title(\"Model Parameters\")\n",
    "plt.ylabel(\"# of Parameters\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "total_parameter_count = sum(parameter_counts.values())\n",
    "\n",
    "for name, count in parameter_counts.items():\n",
    "    print(f\"{name:20s} {count:20,d} {count / total_parameter_count * 100:10.2f}%\")\n",
    "\n",
    "\n",
    "print(f\"Total parameters: {total_parameter_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the \"shape\" of our neural network we'll look at the ratio of embeding dimensions to number of layers. Generally, a aspect ratio betweem 50 and 100 is considered optimal according to certain scaling laws (Kaplan et al., 2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network has an aspect ratio of 64.00\n"
     ]
    }
   ],
   "source": [
    "aspect_ratio = embedding_dimensions / num_hidden_layers\n",
    "\n",
    "print(f\"Network has an aspect ratio of {aspect_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same abalysis for the ratio of embedding dimensions to the number of attention heads. A ratio between 20 and 80 is considered optimal according to the same paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads ratio is 64.00\n"
     ]
    }
   ],
   "source": [
    "heads_ratio = embedding_dimensions / num_attention_heads\n",
    "\n",
    "print(f\"Heads ratio is {heads_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll estimate the size of the model in memory and on disk. Note that this does not include any intermediate variables that get memorized during training such as activations, gradients, optimizer state, and temporary buffers. Actual memory consumption will likely be much higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gigabytes: 0.49G\n"
     ]
    }
   ],
   "source": [
    "bytes_per_parameter = 32 // 8  # Assuming 32-bit floating point\n",
    "\n",
    "total_bytes = total_parameter_count * bytes_per_parameter\n",
    "\n",
    "total_gigabytes = total_bytes / 1e9\n",
    "\n",
    "print(f\"Total gigabytes: {total_gigabytes:,.2f}G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the optimal number of training tokens using the Chinchilla scaling laws given the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal training tokens: 2,471,024,640\n",
      "Epochs required: 2,357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_training_tokens = 20 * total_parameter_count\n",
    "samples_per_epoch = 512\n",
    "tokens_per_sample = 2048\n",
    "num_epochs_required = round(\n",
    "    num_training_tokens / (samples_per_epoch * tokens_per_sample)\n",
    ")\n",
    "print(f\"Optimal training tokens: {num_training_tokens:,}\")\n",
    "\n",
    "print(f\"Epochs required: {num_epochs_required:,}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll estimate the maximum number of floating point operations (FLOPs) required to perform a full forward pass of the nwtwork on a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention                 270,582,939,648      40.96%\n",
      "MLP                       231,928,455,168      35.11%\n",
      "RMS Norm                           69,888       0.00%\n",
      "Output Layer              158,094,852,096      23.93%\n",
      "\n",
      "\n",
      "Total forward FLOPs: 660,606,316,800\n"
     ]
    }
   ],
   "source": [
    "ops_per_matmul = 2 # Multiply + accumulate (MAC)\n",
    "# ops_per_activation = 5 # Assuming SiLU\n",
    "ops_per_activation = 6 # Assuming GeLU\n",
    "ops_per_rms_norm = 7 # y = x / sqrt(sum(x^2) / n + epsilon) * gamma\n",
    "head_dimensions = embedding_dimensions // num_attention_heads\n",
    "\n",
    "# K, Q, V projections\n",
    "attention = (\n",
    "    ops_per_matmul\n",
    "    * tokens_per_sample\n",
    "    * (embedding_dimensions * 3 * embedding_dimensions)\n",
    ")\n",
    "\n",
    "# Attention logits\n",
    "attention += (\n",
    "    ops_per_matmul * tokens_per_sample * tokens_per_sample * embedding_dimensions\n",
    ")\n",
    "\n",
    "# Reductions\n",
    "attention += (\n",
    "    ops_per_matmul\n",
    "    * num_attention_heads\n",
    "    * (tokens_per_sample * tokens_per_sample * head_dimensions)\n",
    ")\n",
    "\n",
    "# Output projection\n",
    "attention += ops_per_matmul * tokens_per_sample * embedding_dimensions**2\n",
    "\n",
    "attention *= num_hidden_layers\n",
    "\n",
    "# Linear transformations\n",
    "mlp = (\n",
    "    ops_per_matmul\n",
    "    * tokens_per_sample\n",
    "    * (embedding_dimensions * (4 * embedding_dimensions))\n",
    ")\n",
    "mlp += (\n",
    "    ops_per_matmul\n",
    "    * tokens_per_sample\n",
    "    * ((4 * embedding_dimensions) * embedding_dimensions)\n",
    ")\n",
    "\n",
    "# Non-linear activations\n",
    "mlp += ops_per_activation * (4 * embedding_dimensions)\n",
    "\n",
    "mlp *= num_hidden_layers\n",
    "\n",
    "rms_norm = ops_per_rms_norm * embedding_dimensions * (num_hidden_layers + 1)\n",
    "\n",
    "output_layer = (\n",
    "    ops_per_matmul * tokens_per_sample * embedding_dimensions * vocabulary_size\n",
    ")\n",
    "\n",
    "flops = {\n",
    "    \"Attention\": attention,\n",
    "    \"MLP\": mlp,\n",
    "    \"RMS Norm\": rms_norm,\n",
    "    \"Output Layer\": output_layer,\n",
    "}\n",
    "\n",
    "plt.bar(flops.keys(), flops.values())\n",
    "\n",
    "plt.title(\"Model Operations\")\n",
    "plt.ylabel(\"# of FLOPs\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "total_forward_flops = sum(flops.values())\n",
    "\n",
    "for name, count in flops.items():\n",
    "    print(f\"{name:20s} {count:20,d} {count / total_forward_flops * 100:10.2f}%\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Total forward FLOPs: {total_forward_flops:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll estimate the number of FLOPs for the backward pass. For this we use a simple heuristic of 2x the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total backward FLOPs: 1,321,212,633,600\n"
     ]
    }
   ],
   "source": [
    "total_backward_flops = 2 * total_forward_flops\n",
    "\n",
    "print(f\"Total backward FLOPs: {total_backward_flops:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same for the total FLOPs per roundtrip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total roundtrip FLOPs: 1,981,818,950,400\n"
     ]
    }
   ],
   "source": [
    "total_roundtrip_flops = total_forward_flops + total_backward_flops\n",
    "\n",
    "print(f\"Total roundtrip FLOPs: {total_roundtrip_flops:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's estimate the number of FLOPs using the method in the PaLM paper by Chowdhery, et al. Then, we'll compare the PaLM estimation with our own as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PaLM FLOPs: 1,982,054,006,784\n"
     ]
    }
   ],
   "source": [
    "palm_flops_per_token = (\n",
    "    6 * total_parameter_count\n",
    "    + 12 * num_hidden_layers * num_attention_heads * head_dimensions * tokens_per_sample\n",
    ")\n",
    "\n",
    "total_palm_flops = palm_flops_per_token * tokens_per_sample\n",
    "\n",
    "print(f\"Total PaLM FLOPs: {total_palm_flops:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two estimates are pretty close so let's proceed.\n",
    "\n",
    "Finally, let's estimate how long it would take to train over the optimal number of tokens given some common Nvidia Ampere generation GPU hardware configurations. Note that these results shown here are a theoretical scenario and do not factor in additional overhead such as activation checkpointing or network latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RTX A2000: 93.41 seconds/epoch, 2.55 days required\n",
      "RTX A4000: 34.81 seconds/epoch, 0.95 days required\n",
      "RTX 3090: 15.45 seconds/epoch, 0.42 days required\n",
      "A100 SXM: 4.39 seconds/epoch, 0.12 days required\n",
      "HGX A100: 0.68 seconds/epoch, 0.02 days required\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Device:\n",
    "    name: str\n",
    "    advertised_flops: float\n",
    "    mfu: float\n",
    "\n",
    "    @property\n",
    "    def actual_flops(self) -> float:\n",
    "        return self.mfu * self.advertised_flops\n",
    "\n",
    "\n",
    "devices = [\n",
    "    Device(\"RTX A2000\", 63.9e12, 0.17),\n",
    "    Device(\"RTX A4000\", 153.4e12, 0.19),\n",
    "    Device(\"RTX 3090\", 285.5e12, 0.23),\n",
    "    Device(\"A100 SXM\", 624.0e12, 0.37),\n",
    "    Device(\"HGX A100\", 4992e12, 0.30),\n",
    "]\n",
    "\n",
    "for device in devices:\n",
    "    seconds_per_epoch = samples_per_epoch * total_roundtrip_flops / device.actual_flops\n",
    "\n",
    "    days_required = num_epochs_required * seconds_per_epoch / 60 / 60 / 24\n",
    "\n",
    "    print(\n",
    "        f\"{device.name}: {seconds_per_epoch:.2f} seconds/epoch, {days_required:,.2f} days required\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch LIghtning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: logs/transformer_experiment\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name      </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> model      DecoderOnlyTransformer   152 M \n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> criterion  CrossEntropyLoss             0 \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName     \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType                  \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\n",
       "\n",
       "\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m model      DecoderOnlyTransformer   152 M \n",
       "\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m criterion  CrossEntropyLoss             0 \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 152 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 152 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 610                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 152 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 152 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 610                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a07eac7b14c48a0be8b2695cfe284da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\Installs\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, RichProgressBar\n",
    "from lightning.pytorch.callbacks.progress.rich_progress import RichProgressBarTheme\n",
    "import tiktoken\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, filepath, tokenizer, block_size=1024):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.tokens = tokenizer.encode(text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokens[idx:idx + self.block_size]\n",
    "        target_ids = self.tokens[idx + 1:idx + self.block_size + 1]\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "# Model\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=768, num_heads=12, num_layers=12, block_size=1024):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(embed_dim, num_heads, dim_feedforward=4 * embed_dim, activation='gelu')\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        # weight sharing\n",
    "        self.token_embedding.weight = self.head.weight\n",
    "\n",
    "        # weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        positions = torch.arange(t, device=x.device).unsqueeze(0)\n",
    "        x = self.token_embedding(x) + self.position_embedding(positions)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x)\n",
    "        x = self.ln(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# Lightning Module\n",
    "class TransformerLightning(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, block_size, lr, warmup_steps, max_steps):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = DecoderOnlyTransformer(vocab_size, block_size=block_size)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, target_ids = batch\n",
    "        logits = self(input_ids)\n",
    "        loss = self.criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "        # Log the loss with 4 decimal precision\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=False, logger=True)\n",
    "        # Optionally print for observation\n",
    "        # self.print(f\"Train Loss: {loss.item():.4f}\")\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < self.hparams.warmup_steps:\n",
    "                return self.hparams.lr * (current_step + 1) / self.hparams.warmup_steps\n",
    "            elif current_step > self.hparams.max_steps:\n",
    "                return self.hparams.lr * 0.1\n",
    "            decay_ratio = (current_step - self.hparams.warmup_steps) / (self.hparams.max_steps - self.hparams.warmup_steps)\n",
    "            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "            return self.hparams.lr * 0.1 + coeff * (self.hparams.lr - self.hparams.lr * 0.1)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "# Training Script\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    block_size = 768\n",
    "    batch_size = 16\n",
    "    max_lr = 6e-4\n",
    "    warmup_steps = 10\n",
    "    max_steps = 25000\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    vocab_size = tokenizer.n_vocab\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    dataset = TextDataset(\"input.txt\", tokenizer, block_size)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Model\n",
    "    model = TransformerLightning(vocab_size, block_size, max_lr, warmup_steps, max_steps)\n",
    "\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    # Set up TensorBoard logger\n",
    "    logger = TensorBoardLogger(\"logs/\", name=\"transformer_experiment\")\n",
    "    # tensorboard --logdir logs/\n",
    "    # create your own theme!\n",
    "    progress_bar = RichProgressBar(\n",
    "            refresh_rate=1, \n",
    "            leave=False, \n",
    "            theme=RichProgressBarTheme(\n",
    "                description='', \n",
    "                progress_bar='#6206E0', \n",
    "                progress_bar_finished='#6206E0', \n",
    "                progress_bar_pulse='#6206E0', \n",
    "                batch_progress='', \n",
    "                time='dim', \n",
    "                processing_speed='dim underline', \n",
    "                metrics='italic', \n",
    "                metrics_text_delimiter=' ', \n",
    "                metrics_format='.3f'), \n",
    "            console_kwargs=None\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_steps=max_steps,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        callbacks=[LearningRateMonitor(logging_interval='step'), progress_bar],\n",
    "        precision='bf16-mixed', # 16-bit floating point, many other options are there\n",
    "        log_every_n_steps=1,\n",
    "        enable_progress_bar=True, # show progress bar\n",
    "        enable_model_summary=True, # show model summary\n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    # https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html\n",
    "    # Training\n",
    "    trainer.fit(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
